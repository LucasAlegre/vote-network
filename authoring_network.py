# -*- coding: utf-8 -*-
"""deputies_prop_themes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W2yP4C2idRQU0mFnBfXHvszNvDC5eOMs
"""

# %%
import argparse
import ntpath
import random
import pandas as pd
import numpy as np
from igraph import Graph, plot, summary
from itertools import combinations
from collections import Counter
from util import draw_vis, pearson_correlation, generalized_similarity, draw_vis_2
import matplotlib.pyplot as plt
import json

node_limit = 100
detection = "multilevel"
weight_threshold = 0.1

"""# Import and prepare dataframe"""

with open("resources/proposicoesAutores-2020.json") as file:
  autores = json.load(file)
with open("resources/proposicoesTemas-2020.json") as file:
  temas = json.load(file)

autores_df = pd.json_normalize(autores["dados"])
temas_df = pd.json_normalize(temas["dados"])

"""## Merge two dataframes by proposition identifier"""

df_autores_temas = pd.merge(autores_df, temas_df, on="uriProposicao", how="inner")

"""## Filter out not productive elements"""


df_autores_temas = df_autores_temas.groupby('nomeAutor').filter(lambda x: len(x) < 3)

"""## Select the number of elements that will be put in the graph based in the limit of nodes"""

all_reps = df_autores_temas["nomeAutor"].unique()
total_reps = len(all_reps)
num_reps = total_reps

if node_limit is not None:
    print("Randomly sampling at most {} representatives".format(node_limit))
    sample_size = min(total_reps, node_limit)
    num_reps = sample_size
    reps = random.sample(list(all_reps), sample_size)

    # Mask to select only those reps that were sampled
    mask = df_autores_temas['nomeAutor'].apply(lambda x: x in reps)
    # Filter out rows for non-sampled reps
    df_autores_temas = df_autores_temas[mask]
else:
    print("Using data from all {} representatives.".format(total_reps))
    reps = all_reps

print(df_autores_temas.head(n=5))

"""## Extracting the themes"""

num_temas = df_autores_temas["tema"].nunique()
temas = df_autores_temas['tema'].unique()
print("Número de temas: ", num_temas)
print("Construindo frafo para {} deputados e {} proposições.".format(num_reps, num_temas))

rep_to_ind = {reps[i]: i for i in range(num_reps)}
temas_to_ind = {temas[i]: i for i in range(num_temas)}
themes = [t for t in df_autores_temas["tema"].unique() if pd.notna(t)]

edges = dict()

authoring_matrix = np.zeros((num_reps, num_temas))

df_grouped = df_autores_temas.groupby(["idProposicao", "nomeAutor"])

for authoring, df_group in df_grouped:
  prop_temas = df_group["tema"].values
  i = rep_to_ind[authoring[1]]
  for tema in prop_temas:
    j = temas_to_ind[tema]
    authoring_matrix[i,j] += 1

print(authoring_matrix)

M = pearson_correlation(authoring_matrix)

for dep1, dep2 in combinations([i for i in range(len(reps))], 2):
    edges[(reps[dep1], reps[dep2])] = M[dep1,dep2]

g = Graph.TupleList([(*pair, weight) for pair, weight in edges.items() if weight > weight_threshold], weights=True)
summary(g)

# Normalize weights to [0,1]
maxw = max(g.es['weight'])
minw = min(g.es['weight'])
g.es['weight'] = [(e - minw) / (maxw - minw) for e in g.es['weight']]

communities = g.community_spinglass(weights='weight', spins=3)

print("Modularity Score: ", g.modularity(communities, 'weight'))

draw_vis_2(g, communities, temas)